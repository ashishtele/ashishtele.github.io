---
layout: single
sidebar: true
author_profile: true
title: "Next-Gen RAG: Innovations and Developments in Retrieval Augmentation Generation"
excerpt: "RAG is a term commonly used in NLP and machine learning to refer to models and techniques that combine retrieval-based methods, augmentation techniques, and text generation to enhance text generation tasks"
description: "RAG is a term commonly used in NLP and machine learning to refer to models and techniques that combine retrieval-based methods, augmentation techniques, and text generation to enhance text generation tasks."
comments: true
tags: ["Resources","Data Scientist","RAG","LLM","Machine Learning","ML System Design"]
published: true
comments: true
header:
  teaserlogo:
  teaser: /images/RAG.png
  image: /images/RAG.png
  caption: "courtesy: neo4j.com"
gallery:

  - image_path: ''
    url: ''
    title: ''
---

I have been experimenting with RAG for a few use cases and I agree with the below statement by LlamaIndex:

```
Prototyping a RAG application is easy, but making it performant, robust, and scalable to a large knowledge corpus is hard.
```
What is Retrieval Augmented Generation (RAG) for LLMs?

Retrieval-augmented generation (RAG) aims to improve prediction quality by using an external datastore at inference time to create an enriched prompt that combines context and relevant knowledge. LLM uses the user input prompt to retrieve external “context” information from a data store that is then included with the user-entered prompt to build a richer prompt containing context information that otherwise would not have been available to the LLM.

However, embedding models typically have a fixed context window so we can generate embeddings on snippets, not entire documents. To use RAG over a set of documents, we have to split (or “chunk”) each document into smaller pieces before converting (“embedding”) them into vectors, where now each embedding corresponds to a specific chunk of information from the original document. When we want to later retrieve information, we search for the K closest embeddings to our query to retrieve the K most relevant text chunks across all of our documents. The retrieved chunks are then used to augment our original prompt and fed to the LLM which processes both the original prompt and the context provided by the chunks to generate a response.
