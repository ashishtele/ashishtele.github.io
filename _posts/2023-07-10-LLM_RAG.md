---
layout: single
sidebar: true
author_profile: true
title: "Next-Gen RAG: Innovations and Developments in Retrieval Augmentation Generation"
excerpt: "RAG is a term commonly used in NLP and machine learning to refer to models and techniques that combine retrieval-based methods, augmentation techniques, and text generation to enhance text generation tasks"
description: "RAG is a term commonly used in NLP and machine learning to refer to models and techniques that combine retrieval-based methods, augmentation techniques, and text generation to enhance text generation tasks."
comments: true
tags: ["Resources","Data Scientist","RAG","LLM","Machine Learning","ML System Design"]
published: true
comments: true
header:
  teaserlogo:
  teaser: /images/RAG.png
  image: /images/RAG.png
  caption: "courtesy: neo4j.com"
gallery:

  - image_path: ''
    url: ''
    title: ''
---

I have been experimenting with RAG for a few use cases and I agree with the below statement by LlamaIndex:

```
Prototyping a RAG application is easy, but making it performant, robust, and scalable to a large knowledge corpus is hard.
```
What is Retrieval Augmented Generation (RAG) for LLMs?

Retrieval-augmented generation (RAG) aims to improve prediction quality by using an external datastore at inference time to create an enriched prompt that combines context and relevant knowledge. LLM uses the user input prompt to retrieve external “context” information from a data store that is then included with the user-entered prompt to build a richer prompt containing context information that otherwise would not have been available to the LLM.

However, embedding models typically have a fixed context window to generate embeddings on snippets, not entire documents. To use RAG over a set of documents, we have to split (or “chunk”) each document into smaller pieces before converting (“embedding”) them into vectors, where now each embedding corresponds to a specific chunk of information from the original document. When we want to retrieve information later, we search for the K closest embeddings to our query to retrieve the K most relevant text chunks across all our documents. The retrieved chunks are then used to augment our original prompt and fed to the LLM which processes both the original prompt and the context provided by the chunks to generate a response.

What are the challenges of simple RAG?

Limited Knowledge: I have observed that limited knowledge is the most common issue. You cannot even recognize it until LLM starts hallucinating, and you attempt to cross-check the answers in the original knowledge base. We can somewhat mitigate the generation of made-up answers by providing context to LLM, but the limited knowledge base will still result in more heuristics. For example, if you load a bunch of contracts and ask a question without a properly constrained prompt, you might receive an incorrect answer.

Retrieval Limitation: The output quality completely depends on the diversity and relevance of retrieved chunks. As I noted, the better the retrieved chunks, the better the generated content. There are many ways of loading and chunking strategies, but most of them are basic and need not necessarily context-aware. Langchain and LlamaIndex do offer different chunking strategies, but still, it is an evolving field. 

Uncertainty with Images and Graphs: Existing retrieval techniques barely support information extraction from images and charts, and while there are some emerging techniques for tabular structure extraction, they remain nebulous.

Complexity and Resource Intensiveness: Implementing RAG methods can be complex and resource-intensive. Managing the integration of information retrieval components with a large language model can be challenging, and it may require substantial computational resources

Noise in Retrieved Information: Information retrieval can introduce noise into the model's input. Irrelevant or contradictory information from the knowledge source can impact the quality of the generated responses, e.g. If you have multiple dates in a contract with the same context, the retrieved chunks can have all the dates and without a properly formatted prompt, the answer can be ambiguous. Albeit it does not fall in the noise category, the contradictory information for sure.

A list of challenges with simple RAG methods can grow quickly as we leverage them more and more for a project. 

Advanced RAG techniques:

Re-Ranking: 

Thank you!
