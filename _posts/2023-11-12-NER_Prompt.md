---
layout: single
sidebar: true
author_profile: true
title: "Prompt Engineering for Named Entity Recognition"
excerpt: "AutoML is one of the more impressive tools in Vertex AI. It allows people with only limited machine learning expertise to create high-quality models"
description: "AutoML is one of the more impressive tools in Vertex AI. It allows people with only limited machine learning expertise to create high-quality models."
comments: true
tags: ["VertexAI","Data Scientist","AUTOML","LLM","Machine Learning","ML System Design"]
published: true
comments: true
header:
  teaserlogo:
  teaser: /images_1/prompt_LLM.PNG
  image: /images_1/prompt_LLM.PNG
  caption: "courtesy: Google.com"
gallery:

  - image_path: ''
    url: ''
    title: ''
---
Hi All,

Prompt engineering refers to the strategic design of crafting a starting text in natural language processing (NLP) tasks to guide models toward desired behavior. The input prompt plays a crucial role in shaping the output generated by the model. The goal is to influence the model's behavior, improve performance, or achieve specific outcomes. 

Why LLM can be a good option for entity extraction out of the box?

### Contextual Understanding:

Large language models capture contextual relationships between words in a text. This contextual understanding is crucial for entity extraction, where the presence and boundaries of entities often depend on the surrounding words and context. This is essential for accurate identification of entities in a sentence.

### Semantic Representations:

Large language models generate high-dimensional semantic representations of words, which encode not only their syntactic but also semantic relationships. This is beneficial for distinguishing between entities and other words based on their meaning within a given context.

### Flexibility Across Domains:

Large language models pre-trained on diverse datasets can handle a wide range of domains and topics. This flexibility is advantageous for entity extraction tasks in different domains without the need for extensive task-specific pre-training. We can even better results from domain-specific models

### Parameter Tuning and Model Size:

LLMs with more parameters can capture intricate patterns in the data. Fine-tuning a large model on a specific entity extraction task often leads to better performance compared to smaller models.

What are the prompting techniques we have at our disposal:
1. Zero-shot prompting
2. Few-shot prompting

## Zero-shot Prompting:
Zero-shot prompting is the method of using natural language prompts to guide pre-trained LLMs on downstream tasks without any gradient updates, fine-tuning of the model, or providing any sample input-output example.

<p align="center">
  <img width="700" height="350" src="/images_1/zero_shot.jpg">
</p>

Our observations with the zero-shot prompting technique for NER:

1. LLMs find it challenging to generalize based solely on context. We have observed that the attribute sequence is neither constant nor reproducible. Additionally, the constraints are loose.
2. As context length increases, the output reproducibility becomes increasingly difficult.
3. LLM does not understand/follow the hard constraints every time.

## Few-shot Prompting:
Few-shot prompting is a technique that leverages the capabilities of Large Language Models (LLMs) to perform specific tasks. By providing a few examples, known as "shots," the model can be conditioned to generate desired outputs, such as text, code, or images.

<p align="center">
  <img width="600" height="450" src="/images_1/few_shot.PNG">
</p>

Our observations with the few-shot prompting technique for NER:

1. A few diverse examples help LLM to learn the attribute sequence.
2. The more examples (shots), the better.
3. Diversity in examples plays an important role.

Stay tuned for more in-depth articles in collaboration with [Vikrant Singh](https://www.linkedin.com/in/vkrntkmrsngh/). You can also visit his [medium articles](https://medium.com/@vkrntkmrsngh)

Thank you!
